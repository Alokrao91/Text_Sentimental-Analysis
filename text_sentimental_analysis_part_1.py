# -*- coding: utf-8 -*-
"""Text/Sentimental Analysis part_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xb25dOFEyfRLm2aa-62HDEkhxV9XueV3

# Part 1

# Pre-processing and EDA
"""

import pandas as pd
import numpy as np
import os
import re
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from nltk.stem.wordnet import WordNetLemmatizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud


pd.set_option('display.max_colwidth',200)

"""# Natural Language Toolkit(nltk)"""

import nltk
nltk.download('all')

"""# Load the tweets dataset"""

tweets= pd.read_csv('/content/drive/MyDrive/data set dp and nlp/new data set/tweets.csv')
tweets.head()

tweets.shape

"""# 0 refers to positive sentiment , 1 is negative sentiment.

## let's map 0 as 1 and 1 as 0
"""

tweets['label']=tweets['label'].map({0:1,1:0})
tweets.head()

tweets.info()

"""# Basic Exploratory Data Analysis"""

# check for missing values
tweets.isnull().sum()

# drop the id column
tweets.drop('id',axis=1,inplace=True)

tweets.head()

tweets['label'].value_counts()

# check for class balance
tweets['label'].value_counts(normalize=True)

tweets['tweet'].unique()

"""# importing seaborn for EDA"""

sns.countplot(data=tweets,x='label')

"""# Data Cleaning"""

tweets['tweet'][24]

tweets['tweet'][14]

"""# regular expression"""

re.sub(r"'s\b"," is",tweets['tweet'][24])

tweets['tweet'][11]

# remove the user mentions
re.sub("@[A-Za-z0-9]+"," ",tweets['tweet'][11])

tweets['tweet'][0]

# remove hastags
re.sub(r"#","",tweets['tweet'][0])

# remove hyperlinks
re.sub(r"https\S+","",tweets['tweet'][0])

tweets['tweet'][25]

# retain only the alphabets(get rid of punct , special char, digits)
re.sub(r"[^a-zA-z]"," ",tweets['tweet'][25])

"""# CONTRACTIONS"""

!pip install contractions
import contractions

contractions.fix("I'm bored")

tweets['tweet'][24]

contractions.fix(tweets['tweet'][24])

"""# stopword removal"""

# from nltk
nltk_stopwords=set(stopwords.words('english'))
print(nltk_stopwords)

len(nltk_stopwords)

"""# english stop words"""

sklearn_stopwords=set(ENGLISH_STOP_WORDS)
print(sklearn_stopwords)

len(sklearn_stopwords)

print(sklearn_stopwords.intersection(nltk_stopwords))

len(sklearn_stopwords.intersection(nltk_stopwords))

"""## combining the stopwords from sklearn & NLTK"""

combined_stopwords=sklearn_stopwords.union(nltk_stopwords)

len(combined_stopwords)

"""# Text Normalization - Stemming or Lemmatization"""

lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('dogs')

tweets['tweet'][63]

tweets['tweet'][63].split()

new_sent = " "
for token in tweets['tweet'][63].split():
  new_sent = new_sent + lemmatizer.lemmatize(token.lower()) + ' '
new_sent

"""# Define a Cleaner Function to handle all type of problem at one go"""

def tweet_cleaner(raw_tweet):
  """this function cleans the raw tweet """
  # substituting contractions
  cleaned_tweet = contractions.fix(raw_tweet)
  # remove the user mentions
  cleaned_tweet = re.sub(r"@[A-Za-z0-9]+","",cleaned_tweet)
  # remove the hastags
  cleaned_tweet = re.sub(r"#","",cleaned_tweet)
  # remove hyperlinks
  cleaned_tweet =re.sub(r"http\S+","",cleaned_tweet)
  # retain only the alphabets (get rid of punct,special char,digits)
  cleaned_tweet =re.sub(r"[^a-zA-Z]"," ",cleaned_tweet)
  cleaned_tweet =cleaned_tweet.lower().strip()
  cleaned_tweet = [token for token in cleaned_tweet.split() if len(token)>2]
  new_sent = ''
  for token in cleaned_tweet:
    new_sent =new_sent + lemmatizer.lemmatize(token) + ' '
  return new_sent.strip()

tweets['tweet'][24]

tweet_cleaner(tweets['tweet'][24])

tweets['cleaned_tweets'] = tweets['tweet'].apply(tweet_cleaner)

tweets.head()

tweets.tail(10)

"""# Data Visialization"""

# create a python list of all the words from all the tweets
vocab_raw = []
for tweet in tweets['tweet']:
  vocab_raw.extend(tweet.split())
print(vocab_raw)

len(vocab_raw)

vocab_raw = [token for tweet in tweets['tweet'] for token in tweet.split()]
print(vocab_raw)

# Frequency distribution of the words
freq_dist = nltk.FreqDist(vocab_raw)
plt.figure(figsize=(12,5))
plt.title("Top 50 Most Common Words", fontsize=15)
plt.xticks(fontsize=15)
freq_dist.plot(50, cumulative=False)
plt.show()

freq_dist

"""# Freq Dist plot for cleaned_tweets"""

vocab_raw = [token for tweet in tweets['cleaned_tweets'] for token in tweet.split()]
print(vocab_raw)

len(vocab_raw)

# Frequency distribution of the words
freq_dist = nltk.FreqDist(vocab_raw)
plt.figure(figsize=(12,5))
plt.title("Top 50 Most Common Words", fontsize=15)
plt.xticks(fontsize=15)
freq_dist.plot(50, cumulative=False)
plt.show()

freq_dist

combined_stopwords

"""# Fre Dist plot for cleaned tweets, after removing stopwords"""

domain_stopwords  = {'twitter','com','io','IO','instagram','facebook','rt'}
domain_stopwords

combined_stopwords = combined_stopwords.union(domain_stopwords)
len(combined_stopwords)

def tweet_cleaner_with_stopwords(raw_tweet):
  """this function cleans the raw tweet """
  # substituting contractions
  cleaned_tweet = contractions.fix(raw_tweet)
  # remove the user mentions
  cleaned_tweet = re.sub(r"@[A-Za-z0-9]+"," ",cleaned_tweet)
  # remove the hastags
  cleaned_tweet = re.sub(r"#"," ",cleaned_tweet)
  # remove the hyperlink
  cleaned_tweet= re.sub(r"http\S+"," ",cleaned_tweet)
  # retain only the alphabets (get rid of punct , special char,digits)
  cleaned_tweet = re.sub(r"[^a-zA-Z]"," ",cleaned_tweet)
  cleaned_tweet = cleaned_tweet.lower().strip()
  # remove stopwords from the new_sent
  cleaned_tweet = [token for token in cleaned_tweet.split() if token not in combined_stopwords]
  # retain only those token which ihave length > 2 characters
  cleaned_tweet = [token for token in cleaned_tweet if len(token)>2]
  new_sent = ''
  for token in cleaned_tweet:
    new_sent = new_sent + lemmatizer.lemmatize(token) + ' '
  return new_sent.strip()

tweets["cleaned_tweets_without_stopwords"] = tweets["tweet"].apply(tweet_cleaner_with_stopwords)
tweets.head()

vocab_raw1 = [token for tweet in tweets['cleaned_tweets_without_stopwords'] for token in tweet.split()]
print(vocab_raw1)

# Frequency distribution of words
freq_dist = nltk.FreqDist(vocab_raw1)
plt.figure(figsize=(12,5))
plt.title("Top 25 Most Common words after removing the stopwords", fontsize=15)
plt.xticks(fontsize=15)
freq_dist.plot(50, cumulative = False)
plt.show()

freq_dist

"""# Create word clouds for better visualization (freq_dist)

"""

!pip install WordCloud
from wordcloud import WordCloud

wordcloud = WordCloud(stopwords= combined_stopwords)
wordcloud.generate_from_frequencies(freq_dist)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

wordcloud = WordCloud(stopwords= combined_stopwords,background_color='blue',\
                      contour_width = 3 ,contour_color='steelblue')
wordcloud.generate_from_frequencies(freq_dist)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# saving the cleaned dataset for further text/sentiment analysis
tweets.to_pickle("cleaned_tweets_v1.pkl")

